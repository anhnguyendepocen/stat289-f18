{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial 19: Text analysis with `gensim`\n",
    "\n",
    "Last week we saw how to use the `XML` module to access and clean the textual\n",
    "data given in the MediaWiki JSON file. Today, we'll se how to use the `gensim`\n",
    "module to actually parse the text itself. Eventually you will do all of this \n",
    "within the `wikitext.py` module and will not need to call `gensim` directly,\n",
    "but it will be helpful to understand how it works for some of the later projects.\n",
    "\n",
    "Start by loading a few functions from the module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, matutils, models, similarities\n",
    "from gensim.similarities.docsim import MatrixSimilarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will work with a small set of text documents rather than\n",
    "the longer wikipedia example. This will make it easier to understand exactly\n",
    "what is going on. Here are the documents we'll use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\"Human machine interface for lab human computer applications\",\n",
    "             \"A survey of user opinion of computer system response time\",\n",
    "             \"The EPS user interface management system\",\n",
    "             \"System and human system engineering testing of EPS\",              \n",
    "             \"Relation of user perceived response time to error measurement\",\n",
    "             \"The generation of random binary unordered trees\",\n",
    "             \"The intersection graph of paths in trees\",\n",
    "             \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "             \"Graph minors A survey\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "The first step in most text processing tasks is to split the raw data into individual\n",
    "words. We have already seen how to do this with regular expressions by detecting sequences\n",
    "of word characaters. Here we also convert the string to lower case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "re.findall('(\\\\w+)', documents[0].lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a list of the words in every document by wrapping this up in a `for`\n",
    "loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = []\n",
    "for doc in documents:\n",
    "    word_list.append(re.findall('(\\\\w+)', doc.lower()))\n",
    "    \n",
    "word_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already seen how these tokenized lists, over longer documents, are\n",
    "useful in detecting the most frequent terms in a given document. Now, we'll\n",
    "see how to do much more with these words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexicon\n",
    "\n",
    "A lexicon, in linguistics proper, refers to all of the terms that a particular\n",
    "person has at their disposal. In the context of text processing, a lexicon \n",
    "refers to all of the terms across of our *corpus* (collection of documents) that\n",
    "we want to consider. To construct a lexicon using `gensim`, we call the `Dictionary`\n",
    "object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon = corpora.Dictionary(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One attribue of the lexicon is a mapping from each term into a numeric id. We\n",
    "can see this with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon.token2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a particular id comes from using the `token2id` attribute as a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon.token2id['human']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To go in the other direction, treat the lexicon as a list and grab an id\n",
    "by reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numeric representation of corpus\n",
    "\n",
    "Consider the first document in our corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using our `lexicon` object it is possible to represent this document a list of integer\n",
    "ids. We could do this through a complex double `for` loop, but `gensim` provides the\n",
    "`doc2idx` function to make this easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon.doc2idx(word_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is this numeric representation useful? For one thing, it takes up considerably\n",
    "less space (at least if we have a large corpus). It is also easier to program with\n",
    "integer ids. This specific representation is useful, as well, if you want to use\n",
    "deep learning models for languages. We may have a chance to see these towards the\n",
    "end of the semester."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words\n",
    "\n",
    "The numeric representation of the terms above does not lose any information in the\n",
    "original document. A bag of words representation, instead, removes all of the order\n",
    "information in a document. It simply counts how often each term in the lexicon occurs\n",
    "within the document. This representation will be essential for nearly all of the methods\n",
    "we will see for processing text.\n",
    "\n",
    "The method `doc2bow` of the `lexicon` converts a list of words into a bag of words.\n",
    "The bag of words is given as a list of tuples given as (word id, count). Here we see,\n",
    "for example, that the first document uses word number 3 twice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon.doc2bow(word_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can cycle through the entire set of documents to create the complete bag of words\n",
    "representation of the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = []\n",
    "for t in word_list:\n",
    "    bow.append(lexicon.doc2bow(t))\n",
    "    \n",
    "bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term frequency matrix\n",
    "\n",
    "A term frequency matrix is an equivalent representation of a bag of words\n",
    "as a matrix, a rectangular table of numeric values. The table has one row\n",
    "for each term in the lexicon and one column for each document (I will also\n",
    "call a matrix with terms in the columns and documents in the rows a term\n",
    "frequency matrix; context should make it clear which one we are working\n",
    "with). The method `corpus2dense` produces such as matrix for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf_array = matutils.corpus2dense(bow, num_terms=len(lexicon.token2id))\n",
    "tf_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notice that this matrix contains a lot of zero elements. The\n",
    "preponderance of zeros only gets worse with larger corpora. We can avoid\n",
    "these by creating instead *sparse* matrix with `corpus2csc`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_sparse_array = matutils.corpus2csc(bow)\n",
    "tf_sparse_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sparse array can (mostly) be manipulated just like the dense version, but saves\n",
    "a lot of space and (sometimes) is much faster to operate on. We won't need either of\n",
    "these term frequency matricies directly until we start building predictive models\n",
    "in a few weeks, but they are a nice way of thinking about the bag of words model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term frequency inverse document frequency\n",
    "\n",
    "If we take the term frequency matrix and determine the largest values in each column,\n",
    "we will get the most frequent terms within each document. We've seen that this can be\n",
    "somewhat useful for understanding the content of a Wikipedia page. However, we have \n",
    "also seen that many of the terms are not very interesting but are instead commonly\n",
    "used across the entire corpus (either grammatic words like \"the\", \"and\", \"because\"\n",
    "or words that would be appear in anything related to our topic such as \"cake\" or \"Virginia\").\n",
    "\n",
    "A solution is to also weight each term as a function of how often it occurs in all\n",
    "documents. There are several ways to do this; I'll explain just the most common\n",
    "technique. Let $f_{t, d}$ count the number of times that term $t$ occurs in document\n",
    "$d$, $n_t$ the number of documents that use term $t$ at least once, and $D$ be the\n",
    "total number of documents. Then, for each term $t$ and document $d$ we have the \n",
    "tf-idf (term frequency inverse document frequency) of:\n",
    "\n",
    "$$ tfidf(t, d) = f_{t, d} \\times log_2 \\left( \\frac{D}{n_t} \\right) $$\n",
    "\n",
    "So, the score will be higher if the term is used more frequently in a document but\n",
    "lower if the term is used in more documents. **The idea is that terms with the highest\n",
    "tfidf score for a given document are the most distinguishing ones for that particular\n",
    "document.**\n",
    "\n",
    "We can build a tfidf model using the `TfidfModel` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model can then be applied to any particular document of interest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf[bow[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most relevant term in document 1 is then term number 9, even\n",
    "though it is used only once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon[9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visually determin the most interesting term for a single document,\n",
    "but it is useful to automatically sort the list for use in general programming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_obj = tfidf[bow[1]]\n",
    "sorted(tf_obj, key=lambda x: x[1], reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we see the top five terms for this particular document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_terms = 5\n",
    "\n",
    "top_terms = []\n",
    "for obj in sorted(tf_obj, key=lambda x: x[1], reverse=True)[:n_terms]:\n",
    "    top_terms.append(\"{0:s} ({1:01.03f})\".format(lexicon[obj[0]], obj[1]))\n",
    "\n",
    "print(top_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to also represent the TF-IDF object as a matrix, as show by\n",
    "the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_corpus = []\n",
    "for doc in bow:\n",
    "    tfidf_corpus.append(tfidf[doc])\n",
    "\n",
    "tfidf_mat = matutils.corpus2dense(tfidf_corpus, num_terms=len(lexicon.token2id))\n",
    "tfidf_mat[:40, :4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the values are weighted according to how frequent the word is across the\n",
    "corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity\n",
    "\n",
    "Consider each column of the term frequency of TF-IDF matrix. This sequence\n",
    "of numbers describes the terms used with a given document. In other words,\n",
    "we have projected each document in a D-dimensional space (where $D$ is the\n",
    "number of terms in our lexicon). Now that our documents are just points in\n",
    "space, there are a lot *mathy* things we can do with them. One example is\n",
    "determining which documents are similar to which other documents. We could\n",
    "just compute the (Euclidean) distance between two documents, but there is\n",
    "another metric that is popular in text analysis called *cosine similarity*.\n",
    "It measures the angle between two documents; the details are not important\n",
    "but what matters is the cosine similarity is not sensitive to how long a\n",
    "particular document is.\n",
    "\n",
    "We can apply cosine similarty to either the TF matrix or the TF-IDF matrix.\n",
    "The latter is generally recommended. Here, we use the `MatrixSimilarity` \n",
    "class to compute a similarity score for our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matsim = MatrixSimilarity(tfidf_corpus, num_features=len(lexicon))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the TF-IDF class, we need to apply this similarity score to a particular document.\n",
    "Here we apply it to document 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matsim[tfidf_corpus[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A cosine similarity score of 1 means that two documents are perfectly identical; notice\n",
    "that document 1 has a score of $0.99999994$ to itself due to rounding errors. The next\n",
    "most similar document is document 8, probably because both use the rare term \"survey\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Document 1: '\" + documents[1] + \"'\")\n",
    "print(\"Document 8: '\" + documents[8] + \"'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the term frequency measurement, we could automate detecting the closest document\n",
    "and use this to explore a corpus of text. And again, we could think of these similarities\n",
    "as a matrix, here a square one with one row and column for each document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.round(matsim[tfidf_corpus], 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that, when rounded, the diagonal elements are all one. Why is this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering\n",
    "\n",
    "Another task we can do given that our documents are projected in a high-dimensional\n",
    "numeric space is to cluster the documents according to the words they use. We have\n",
    "already seen how to do this with the network data; we are just doing this same thing\n",
    "now with the words themselves.\n",
    "\n",
    "Due to something called the *curse of dimensionality*, applying classical clustering\n",
    "techniques to our data will not work very well. There are too many dimensions, the \n",
    "number of words in our lexicon, to reasonably do any clustering of the documents.\n",
    "One clustering technique that does work very well works directly with a matrix of\n",
    "similarity scores; it is called *spectral clustering* and we can apply it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "\n",
    "scmodel = SpectralClustering(n_clusters=3, affinity='precomputed')\n",
    "similarity_matrix = matsim[tfidf_corpus]\n",
    "scmodel.fit_predict(similarity_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The spectral clustering splits our documents into three groups and returns the id\n",
    "of each group. If you want to know more about the clustering algorithm itself, I'll\n",
    "put some notes on the main course site. It's hard to explain the technique unless \n",
    "you've had linear algebra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Models\n",
    "\n",
    "A topic model is a probabilistic technique for understanding *topics* that occur\n",
    "within a corpus. Here, topics are understood as groups of words that tend to co-occur\n",
    "within documents. For example, the words 'flour', 'oil', 'sugar', and 'oven' may\n",
    "all group together to form a topic about baking. By far the most popular technique\n",
    "for detecting topics is an approach called Latent Dirchlet Allocation, or LDA.\n",
    "Here we will use `gensim` to apply the model to our corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "lda = LdaModel(bow, id2word=lexicon, num_topics=3, alpha='auto', iterations=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the most highly weighted words for each of the three topics (this corpus\n",
    "is far too small for the output to really make much sense):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.show_topics(num_words=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise, we can see how much of each document is associated with each topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(lda.get_document_topics(bow))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have also added a reference on the course website about LDA that does not require\n",
    "understanding all of the underlying probability theory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords and lexicon creation\n",
    "\n",
    "Our final topic for this tutorial is how to manually reduce the number of \n",
    "terms in our lexicon. There are two reasons we may want to remove a term\n",
    "from the lexicon:\n",
    "\n",
    "1. Terms that occur too frequently, or are *function* words that serve a\n",
    "primarly grammatical function, may result in erroneous results particularly\n",
    "in our topic models.\n",
    "2. There will be a large number of occurs that only occur a very small number\n",
    "of documents. These are generally not very interesting (mispelled words or\n",
    "proper names) and can be removed to save time and space. They also may cause\n",
    "issues in TF-IDF as they become too heavily weighted.\n",
    "\n",
    "There is an easy method `filter_extremes` that removes terms based on how \n",
    "frequently they are used in the corpus. Here we keep only those terms that\n",
    "are used in at least two documents and in no more than 70% of the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(lexicon))\n",
    "lexicon.filter_extremes(no_below=2, no_above=0.7)\n",
    "print(len(lexicon))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lexicon now has only 16 terms from the original 41. We can also use a list\n",
    "of pre-defined terms that we want to remove, known as stopwords. Here is a common\n",
    "list of English terms that we will make use of this semester:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ranksnl_large.txt', 'r') as fin:\n",
    "    sw_list = fin.read().splitlines()\n",
    "    \n",
    "print(sw_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code removes these terms from our lexicon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_list = set(sw_list).intersection(lexicon.token2id.keys())\n",
    "ids = [lexicon.token2id[x] for x in sw_list]\n",
    "lexicon.filter_tokens(ids)\n",
    "len(lexicon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result now contains four fewer terms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
