{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial 26: Unsupervised learning\n",
    "\n",
    "So far, we have seen how to perform supervised learning tasks. That is, building\n",
    "predictive models. Another task in machine learning is *unsupervised learning*,\n",
    "where we want to learn features of a dataset without a particular variable that\n",
    "we are trying to uncover. There are two common classes of unsupervised learning:\n",
    "\n",
    "1. clustering: breaking the input data into groups; we have already seen this from\n",
    "both network analysis and spectral clustering on the words\n",
    "2. dimensionality reduction: taking a dataset with many variables and producing a\n",
    "new dataset with a smaller number of variables that capture the most dominant\n",
    "features of the original space.\n",
    "\n",
    "In this tutorial, we focus on the task of dimensionality reduction. It will be \n",
    "useful for term frequency matrices and also leads nicely into next week's introduction\n",
    "to neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load modules and data\n",
    "\n",
    "To start, let's load in a number of modules that will be useful for the tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wiki\n",
    "import iplot\n",
    "import wikitext\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (20.0, 10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert wiki.__version__ >= 6\n",
    "assert wikitext.__version__ >= 3\n",
    "assert iplot.__version__ >= 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For one last time, let's make use of the list of novelists and poets dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "data = wiki.get_wiki_json(\"List_of_American_novelists\")\n",
    "data_html = data['text']['*']\n",
    "authors = re.findall('<li><a href=\"/wiki/([^\"]+)\"', data_html)\n",
    "nov_authors = authors[:(authors.index('Leane_Zugsmith') + 1)]\n",
    "\n",
    "data = wiki.get_wiki_json(\"List_of_poets_from_the_United_States\")\n",
    "data_html = data['text']['*']\n",
    "authors = re.findall('<li><a href=\"/wiki/([^\"]+)\"', data_html)\n",
    "poe_authors = authors[:(authors.index('Louis_Zukofsky') + 1)]\n",
    "\n",
    "nov_authors = list(set(nov_authors) - set(poe_authors))\n",
    "poe_authors = list(set(poe_authors) - set(nov_authors))\n",
    "links = nov_authors + poe_authors\n",
    "\n",
    "y_vals = np.array([0] * len(nov_authors) + [1] * len(poe_authors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wcorp = wikitext.WikiCorpus(links, num_clusters=15, num_topics=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And grab the term-frequency matrix and list of words. We will also produce a new \n",
    "version of the term frequency matrix that *normalizes* the size of each column. This\n",
    "is similar to the TF-IDF matrix we constructed several weeks ago."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_mat = wcorp.sparse_tf().transpose()\n",
    "tf_norm = normalize(tf_mat, norm='l2', axis=1)\n",
    "words = wcorp.terms()\n",
    "\n",
    "print(tf_norm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal components\n",
    "\n",
    "One of the most common methods for dimensionality reduction is a method called principal\n",
    "component analysis, or PCA. If you are familiar with matrix algebra, it is closely related\n",
    "to the concept of the singular value decomposition (SVD). Here is how to use the **sklearn**\n",
    "module to project a dataset using the PCA (or, as they call it here, the SVD):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = sklearn.decomposition.TruncatedSVD(n_components=2)\n",
    "embed = pca.fit_transform(tf_norm)\n",
    "print(embed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that our original dataset with over 18k columns has been converted into a new dataset\n",
    "with only two columns. Let's visualize what these columns measure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dict(xval = embed[:, 0],\n",
    "                       yval = embed[:, 1],\n",
    "                       link = wcorp.meta.link,\n",
    "                       title = wcorp.meta.title,\n",
    "                       num_sections = wcorp.meta.num_sections,\n",
    "                       poet_novel = y_vals))\n",
    "\n",
    "fig = iplot.create_figure(df, 'xval', 'yval', url='link', color='poet_novel')\n",
    "iplot.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that, without any direct knowledge of the distinction between poets and\n",
    "novelists, the plot naturally seperates these two. Click on a few of the points\n",
    "that are clustered in the \"wrong\" place; what do you notice?\n",
    "\n",
    "Coloring by the number of sections shows the other side of what is captured in the\n",
    "embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dict(xval = embed[:, 0],\n",
    "                       yval = embed[:, 1],\n",
    "                       link = wcorp.meta.link,\n",
    "                       title = wcorp.meta.title,\n",
    "                       num_sections = wcorp.meta.num_sections,\n",
    "                       poet_novel = y_vals))\n",
    "\n",
    "fig = iplot.create_figure(df, 'xval', 'yval', url='link', color='num_sections')\n",
    "iplot.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Roughly, the dimension perpendicular to the poet/novelist distinction measures how long\n",
    "the article is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-Distributed Stochastic Neighbor Embedding\n",
    "\n",
    "There also exist more complex, non-linear, embeddings that can help to capture\n",
    "other features in high-dimensional data. One popular example is known as *t-SNE*.\n",
    "For computational reasons, we need to first project into a relatively high dimensional\n",
    "PCA projection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = sklearn.decomposition.TruncatedSVD(n_components=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = pca.fit_transform(tf_norm)\n",
    "embed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then, re-project these 50-dimensions into 2 using the non-linear technique:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = sklearn.manifold.TSNE(perplexity=25, n_iter=300)\n",
    "tembed = tsne.fit_transform(embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the results are more evenly distributed across the two-dimensional\n",
    "space but still capture the poet/novelist distinction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dict(xval = tembed[:, 0],\n",
    "                       yval = tembed[:, 1],\n",
    "                       link = wcorp.meta.link,\n",
    "                       title = wcorp.meta.title,\n",
    "                       poet_novel = y_vals))\n",
    "\n",
    "fig = iplot.create_figure(df, 'xval', 'yval', url='link', color='poet_novel')\n",
    "iplot.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll admit, this is not a very convincing example for doing the extra work of running t-SNE.\n",
    "But, there are many instances (image analysis, for example) where the results are extremely\n",
    "more understandble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
