{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial 27: Neural networks, deep learning, and keras\n",
    "\n",
    "In this tutorial, you will get a very basic introduction to neural networks\n",
    "and how to build them in Python. Let us start by loading all of our standard\n",
    "modules and scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wiki\n",
    "import iplot\n",
    "import wikitext\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert wiki.__version__ >= 6\n",
    "assert wikitext.__version__ >= 2\n",
    "assert iplot.__version__ >= 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For today, we will once again take links from the \"important publications in\n",
    "philosophy\" page to build a corpus for prediction. We will make a `WikiCorpus`\n",
    "object to simplify the computation of metrics for the page. Below I have removed\n",
    "two pages that give our Windows users some trouble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "links = wikitext.get_internal_links('List_of_important_publications_in_philosophy')['ilinks']\n",
    "links.remove(\"What_Is_it_Like_to_Be_a_Bat?\")\n",
    "links.remove(\"What_is_Life?_(Schr√∂dinger)\")\n",
    "links = np.random.permutation(links)\n",
    "wcorp = wikitext.WikiCorpus(links, num_clusters=15, num_topics=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, again, we will grab two potential response variables (one continuous variable\n",
    "and one categorical one) and stack them together in a numeric numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ilinks = wcorp.meta['num_ilinks'].values\n",
    "lan_version = np.array(['ru' in x for x in wcorp.meta['langs']], dtype=np.int)\n",
    "\n",
    "num_sections = wcorp.meta['num_sections'].values\n",
    "num_images = wcorp.meta['num_images'].values\n",
    "num_elinks = wcorp.meta['num_elinks'].values\n",
    "num_langs = wcorp.meta['num_langs'].values\n",
    "num_chars = np.array([len(x) for x in wcorp.meta['doc'].values])\n",
    "\n",
    "x = np.stack([num_sections, num_images, num_elinks, num_langs, num_chars], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural networks\n",
    "\n",
    "Neural networks, or deep learning, is often made to sound like a fancy, scary,\n",
    "impossible to understand thing. I try to think of them as just another way of\n",
    "building a predictive model (albeit, an important one). I cannot go into too\n",
    "much detail given the time-constraint, but let's talk about the basic idea of\n",
    "a small neural network: its a sequence of chained together linear models.\n",
    "\n",
    "What's the benefit of putting together multiple linear models? Think of this\n",
    "very simple description of a single input (x) a single output (y) and one single\n",
    "\"hidden\" layer with two \"hidden\" parameters (z1 and z2):\n",
    "\n",
    "<img src=\"img/nn2.png\" alt=\"drawing\" width=\"740\"/>\n",
    "\n",
    "You'd be correct in thinking this is silly. Any feasible output y could be\n",
    "described directly without requiring these two hidden values. Visually, we can\n",
    "see that any combination of two linear models just gives another linear model:\n",
    "\n",
    "<img src=\"img/nn1.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "However, we have one minor modification to make. Rather than using the raw output\n",
    "of the linear regressions (z1 and z2), we apply a function called a Rectified\n",
    "Linear Unit, or ReLU. It is a really fancy name of taking the positive part of\n",
    "the function. If we do this, then we can get a non-linear output y from a chain\n",
    "of linear models:\n",
    "\n",
    "<img src=\"img/nn3.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "In fact, it turns out, with enough hidden layers a neural network is a universal\n",
    "function approximator. That is, it can approximate any (reasonably smooth)\n",
    "function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building deep learning models\n",
    "\n",
    "To start actually building a neural network, we need a few functions from keras.\n",
    "Let's load them in here: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will see how to build predictive models using neural networks and the\n",
    "**keras** module. As a starting point, we need to normalize the data matrix \n",
    "x so that each column has unit norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = normalize(x)\n",
    "y = normalize(num_ilinks).transpose()\n",
    "\n",
    "y_train = y[:325, :]\n",
    "y_test  = y[325:, :]\n",
    "x_train = x[:325, :]\n",
    "x_test  = x[325:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build the actual model, we start with an empty sequential model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then add an *input* layer. This tells keras how many columns\n",
    "are in x and how many hidden z's we want in the first layer. Let's\n",
    "just use 2 hidden values like our toy example. Notice that I set\n",
    "the 'relu' activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(units=2, activation='relu', input_dim=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I'll add the output layer. Our response value y has only a\n",
    "single column, so this layer just has one unit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(units=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the entire model by printing out the model summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before trying to learn the parameters in the model from our training data,\n",
    "we need to *compile* the layers. This makes it much faster to train when\n",
    "using large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', optimizer='sgd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can fit the data using our training data. Keras allows us\n",
    "to directly pass the validation data to see how well the function works.\n",
    "Note that this algorithm does not have a specific analytic solution and\n",
    "requires us to simulate the solution. This is what the *epochs* parameter\n",
    "controls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction works similar to the sklearn functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred = model.predict(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can see what the actual weights are as follows. Here are the\n",
    "weights from the first layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "And here are the weights from the second layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[1].get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A deeper model\n",
    "\n",
    "We can construct much larger and deeper models using keras. Here is\n",
    "a model with four hidden layers with 32 hidden states in each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units=32, activation='relu', input_dim=5))\n",
    "model.add(Dense(units=32, activation='relu'))\n",
    "model.add(Dense(units=32, activation='relu'))\n",
    "model.add(Dense(units=32, activation='relu'))\n",
    "model.add(Dense(units=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', optimizer='sgd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, epochs=25, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural networks for classification\n",
    "\n",
    "We can, and more often than not do, build neural networks for classification\n",
    "tasks. The easiest way to make this happen is by converting a categorical output\n",
    "to a *one-hot encoding* by building a matrix with one column per category. This\n",
    "can be done with the `to_categorical` function from keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(lan_version)\n",
    "y[:10,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then split this into a training and testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y[:325, :]\n",
    "y_test  = y[325:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we build a neural network we need to make two changes:\n",
    "first, the final layer needs to have two units, and secondly the\n",
    "final layer needs a special activation function. The special\n",
    "activation function is called a \"softmax\" and ensures that the\n",
    "two values are positive numbers that add up to one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units=32, activation='relu', input_dim=5))\n",
    "model.add(Dense(units=2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also use some different parameters in the model compilation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='RMSprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the model works exactly the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "I'll admit that neither of these toy problems work very well with neural\n",
    "networks. These are not the kinds of problems designed to work well with\n",
    "them. I will also admit that we really do not have the kind of time (nor\n",
    "can I assume that mathematical background) needed to really learn about\n",
    "how to build neural networks in MATH289. I hope, though, that you get\n",
    "something out of these notes. We will be using neural networks in the next\n",
    "tutorial and I think you'll find them, in that form, quite accessable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
