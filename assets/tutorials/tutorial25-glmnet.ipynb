{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial 25: Predicting with Words â€” The Elastic Net\n",
    "\n",
    "Today, we will learn how to build predictive models that classify textual\n",
    "documents by the words used in the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading modules\n",
    "\n",
    "Start by loading our standard modules and make sure that everything is working\n",
    "as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wiki\n",
    "import iplot\n",
    "import wikitext\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "\n",
    "import glmnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (20.0, 10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert wiki.__version__ >= 6\n",
    "assert wikitext.__version__ >= 3\n",
    "assert iplot.__version__ >= 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "Our dataset here will be the pages linkined to from the\n",
    "[List of American novelists](https://en.wikipedia.org/wiki/List_of_American_novelists) and\n",
    "[List of poets from the United States](https://en.wikipedia.org/wiki/List_of_poets_from_the_United_States).\n",
    "Get the pages by grabing the bulk download from my website to speed things up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wiki.bulk_download('novel-poem', force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below constructs seperate lists of novelists and poets, making sure\n",
    "to remove anyone on both lists. Finally it constructs an output vector `y_vals`\n",
    "that is 0 for authors and 1 for poets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "data = wiki.get_wiki_json(\"List_of_American_novelists\")\n",
    "data_html = data['text']['*']\n",
    "authors = re.findall('<li><a href=\"/wiki/([^\"]+)\"', data_html)\n",
    "nov_authors = authors[:(authors.index('Leane_Zugsmith') + 1)]\n",
    "\n",
    "data = wiki.get_wiki_json(\"List_of_poets_from_the_United_States\")\n",
    "data_html = data['text']['*']\n",
    "authors = re.findall('<li><a href=\"/wiki/([^\"]+)\"', data_html)\n",
    "poe_authors = authors[:(authors.index('Louis_Zukofsky') + 1)]\n",
    "\n",
    "nov_authors = list(set(nov_authors) - set(poe_authors))\n",
    "poe_authors = list(set(poe_authors) - set(nov_authors))\n",
    "links = nov_authors + poe_authors\n",
    "\n",
    "y_vals = np.array([0] * len(nov_authors) + [1] * len(poe_authors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, create a `wcorp` object to wrap up all of the information we need for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wcorp = wikitext.WikiCorpus(links, num_clusters=15, num_topics=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Textual training data\n",
    "\n",
    "Recall that the `WikiCorpus` object has a function for returning the term\n",
    "frequency matrix. Here, we grab the sparse version of the matrix because it\n",
    "is much smaller and can be passed directly to most sklearn algorithms. Here,\n",
    "it should have over 18k rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_mat = wcorp.sparse_tf().transpose()\n",
    "tf_mat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, it will be useful to grab the names of the words in each column\n",
    "(here, we print out the first 100 terms):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = wcorp.terms()\n",
    "words[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, consider using the matrix `tf_mat` in a predictive model. Here it has 18k+ columns; \n",
    "in general, it is impossible to learn 18k parameters (as in a linear regression) \n",
    "with only 2800 observations. We need a method that is able to handle such models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic net\n",
    "\n",
    "Consider a simple linear regression model. We have mentioned that the the ordinary\n",
    "least squares estimator is defined by minimizing the sum of squared residuals:\n",
    "    \n",
    "$$ \\text{LEAST SQUARES} \\rightarrow \\arg\\min_{a, b} \\left\\{ \\sum_i \\left( y_i - a - b \\cdot x_i \\right)^2  \\right\\}$$\n",
    "\n",
    "The lasso estimator modifies this slightly by adding a *penalty* term the entices\n",
    "the model to make the slove parameter smaller:\n",
    "\n",
    "$$ \\text{LASSO} \\rightarrow \\arg\\min_{a, b} \\left\\{ \\sum_i \\left( y_i - a - b \\cdot x_i \\right)^2 + \\lambda \\cdot | b |  \\right\\}$$\n",
    "\n",
    "For multivariate data, this becomes (for those familiar with vector norms):\n",
    "\n",
    "$$ \\text{LASSO} \\rightarrow \\arg\\min_{\\beta} \\left\\{ || y - \\beta X ||_2^2 + \\lambda \\cdot || b ||_1  \\right\\}$$\n",
    "\n",
    "And finally, the elastic net is given by:\n",
    "\n",
    "$$ \\text{ELASTIC NET} \\rightarrow \\arg\\min_{\\beta} \\left\\{ || y - \\beta X ||_2^2 +\n",
    "   \\lambda \\cdot \\rho || b ||_1 + \\lambda \\cdot (1 - \\rho) || b ||_2^2  \\right\\}$$\n",
    "\n",
    "The details for us in this course are not important; what should be taken away is that we\n",
    "have a model that forces slope parameters to be zero unless they are particularly useful\n",
    "in the prediction task. It turns out that this is particularly useful for text prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a logistic elasic net according to the same approach used in other sklearn\n",
    "estimators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lnet = glmnet.LogitNet()\n",
    "lnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, as with other estimators, we fit the data using the `fit` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lnet.fit(tf_mat, y_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well as constructing predictions using the predict function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lnet.predict(tf_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And see how well it performs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.accuracy_score(y_vals, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the selected parameters\n",
    "\n",
    "More than the model itself, though, the most interesting thing about the elastic\n",
    "net is seeing what variables were choosen by the algorithm. To start, it is helpful\n",
    "to wrap up a list that matches each word to its coefficent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = list(zip(words, lnet.coef_[0, :]))\n",
    "vals[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then, sort the results by the coefficent and show the non-zero\n",
    "values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in sorted(vals, key=lambda x: x[1], reverse=True):\n",
    "    if x[1] != 0:\n",
    "        print(\"{0:15s} => {1: 8.2f}\".format(x[0], x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that poets are coded as 1's and novelists as 0's;\n",
    "so positive terms are correlated with poets and negatives are novelists.\n",
    "Do the results make sense to you?\n",
    "\n",
    "The value for $\\lambda$ in the elastic net is choosen by trying up to 100\n",
    "values and using a technique for determining which one is best. Sometimes \n",
    "it is also useful to look at non-optimal values, for example if the optimal\n",
    "output contains too many or too few terms to understand the structure of\n",
    "the data. Here, we grab the 30th largest value of the tuning parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = list(zip(words, lnet.coef_path_[0, :, 26]))\n",
    "for x in sorted(vals, key=lambda x: x[1], reverse=True):\n",
    "    if x[1] != 0:\n",
    "        print(\"{0:15s} => {1: 8.2f}\".format(x[0], x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the new values make sense to you? Do any seem superfluous?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another application\n",
    "\n",
    "Let's use the same dataset applied to a different respones variable: whether\n",
    "the page has been translated into German. Note that positive values are associated\n",
    "with translated pages and negative values are not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lan_version = np.array(['de' in x for x in wcorp.meta['langs']], dtype=np.int)\n",
    "\n",
    "lnet = glmnet.LogitNet()\n",
    "lnet.fit(tf_mat, lan_version)\n",
    "\n",
    "vals = list(zip(words, lnet.coef_[0, :]))\n",
    "for x in sorted(vals, key=lambda x: x[1], reverse=True):\n",
    "    if x[1] != 0:\n",
    "        print(\"{0:15s} => {1: 8.2f}\".format(x[0], x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, for French:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lan_version = np.array(['fr' in x for x in wcorp.meta['langs']], dtype=np.int)\n",
    "\n",
    "lnet = glmnet.LogitNet()\n",
    "lnet.fit(tf_mat, lan_version)\n",
    "\n",
    "vals = list(zip(words, lnet.coef_[0, :]))\n",
    "for x in sorted(vals, key=lambda x: x[1], reverse=True):\n",
    "    if x[1] != 0:\n",
    "        print(\"{0:15s} => {1: 8.2f}\".format(x[0], x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And once more, in Chinese:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lan_version = np.array(['zh' in x for x in wcorp.meta['langs']], dtype=np.int)\n",
    "\n",
    "lnet = glmnet.LogitNet()\n",
    "lnet.fit(tf_mat, lan_version)\n",
    "\n",
    "vals = list(zip(words, lnet.coef_[0, :]))\n",
    "for x in sorted(vals, key=lambda x: x[1], reverse=True):\n",
    "    if x[1] != 0:\n",
    "        print(\"{0:15s} => {1: 8.2f}\".format(x[0], x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What patterns do you see in the data here? **Does it tell you anything about the\n",
    "nature of Wikipedia?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also found predicting whether a page has more than 2 images to be similarly\n",
    "interesting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_flag = wcorp.meta['num_images'].values > 2\n",
    "\n",
    "lnet = glmnet.LogitNet()\n",
    "lnet.fit(tf_mat, image_flag)\n",
    "\n",
    "vals = list(zip(words, lnet.coef_[0, :]))\n",
    "for x in sorted(vals, key=lambda x: x[1], reverse=True):\n",
    "    if x[1] != 0:\n",
    "        print(\"{0:15s} => {1: 8.2f}\".format(x[0], x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Any take aways from this set of words?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Practice\n",
    "\n",
    "Using the data that you had for Project 3 and 4, run a predictive model on the \n",
    "textual data and print out the most significant terms for some item of interest.\n",
    "If your set was put together by two different selections, use that. Otherwise,\n",
    "try to predicte a certain type of translation, number of images, or other metric\n",
    "that seems interesting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
