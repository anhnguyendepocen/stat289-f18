{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial 10: Lists and Loops\n",
    "\n",
    "Today we will learn about using lists and for loops in Python.\n",
    "This will allow us to use the MediaWiki API function to grab\n",
    "data from several websites.\n",
    "\n",
    "Let's start by importing the modules we will need at the functions\n",
    "for loading JSON objects in Python. Notice that I've made a few\n",
    "small changes to the code to make it function a bit better for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "import urllib\n",
    "\n",
    "from os.path import join\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "def wiki_json_path(page_title, lang='en'):\n",
    "    \"\"\"Returns local path to JSON file for Wikipeida page data\n",
    "    \n",
    "    This function is used to determine where the dump of a \n",
    "    call to the MediaWiki API, using the parse method, should\n",
    "    be stored. As an extra action, the function also checks that\n",
    "    the relevant directory exists and creates it if it does not.\n",
    "    \n",
    "    Args:\n",
    "        page_title: A string containing the page title.\n",
    "        lang: Two letter language code describing the Wikipedia\n",
    "            language used to grab the data.\n",
    "            \n",
    "    Returns:\n",
    "        A string describing a relative path to file.\n",
    "    \"\"\"\n",
    "    page_title = re.sub(\"[ /]\", \"_\", page_title)\n",
    "    stat289_base_dir = os.path.dirname(os.getcwd())\n",
    "    \n",
    "    dir_name = join(stat289_base_dir, \"data\", lang)\n",
    "    if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "        \n",
    "    file_name = page_title + \".json\"\n",
    "    file_path = join(dir_name, file_name)\n",
    "    \n",
    "    return file_path\n",
    "\n",
    "\n",
    "def get_mediawiki_request(page_title, lang):\n",
    "    \"\"\"Returns URL to make parse request to the MediaWiki API\n",
    "        \n",
    "    Args:\n",
    "        page_title: A string containing the page title.\n",
    "        lang: Two letter language code describing the Wikipedia\n",
    "            language used to grab the data.\n",
    "            \n",
    "    Returns:\n",
    "        A string giving the complete request URL.\n",
    "    \"\"\"\n",
    "    page_title = re.sub(\" \", \"_\", page_title)\n",
    "    page_title = urllib.parse.urlencode({'page': page_title})\n",
    "    page_title = re.sub('%2F', '/', page_title) # don't encode '/'\n",
    "    \n",
    "    base_api_url = 'https://' + lang + '.wikipedia.org/w/api.php'\n",
    "    default_query = 'action=parse&format=json&'\n",
    "\n",
    "    url = base_api_url + \"?\" + default_query + page_title\n",
    "    return url\n",
    "\n",
    "\n",
    "def get_wiki_json(page_title, lang='en'):\n",
    "    \"\"\"Returns JSON data as a dictionary for the Wikipedia page\n",
    "    \n",
    "    This function either loads a cached version of the page or,\n",
    "    if a local version of the page is not available, calls the\n",
    "    MediaWiki API directly.\n",
    "    \n",
    "    Args:\n",
    "        page_title: A string containing the page title.\n",
    "        lang: Two letter language code describing the Wikipedia\n",
    "            language used to grab the data.\n",
    "            \n",
    "    Returns:\n",
    "        A dictionary object with the complete parsed JSON data.\n",
    "    \"\"\"\n",
    "    file_path = wiki_json_path(page_title, lang)\n",
    "    \n",
    "    # if page does not exist, grab it from Wikipedia\n",
    "    if not os.path.exists(file_path):\n",
    "        print(\"Pulling data from MediaWiki API: '\" + page_title + \"'\")\n",
    "        url = get_mediawiki_request(page_title, lang)\n",
    "        r = requests.get(url)\n",
    "        if r.status_code != requests.codes.ok:\n",
    "             raise IOError('Website cannot be reached')\n",
    "        page_data = r.json()\n",
    "        if 'parse' not in page_data:\n",
    "            raise IOError('Wikipedia page not found')\n",
    "        with open(file_path, 'w') as outfile:\n",
    "            json.dump(page_data['parse'], outfile)\n",
    "        time.sleep(0.5) # sleep for half second to avoid API limits\n",
    "            \n",
    "    # read the JSON data from local filesystem\n",
    "    with open(file_path, 'r') as infile:\n",
    "        new_data = json.load(infile)\n",
    "    \n",
    "    return new_data\n",
    "\n",
    "\n",
    "def links_as_list(data):\n",
    "    \"\"\"Extracts MediaWiki JSON links as a list object.\n",
    "    \n",
    "    This helper function extracts valid MediaWiki links from\n",
    "    Wikipedia and returns the data as a list.\n",
    "    \n",
    "    Args:\n",
    "        data: Data returned from the function get_wiki_json.\n",
    "            \n",
    "    Returns:\n",
    "        A list containing the internal links on the page.\n",
    "    \"\"\"\n",
    "    output = []\n",
    "    for link in data['links']:\n",
    "        if link['ns'] == 0 and 'exists' in link:\n",
    "            output.append(link['*'])\n",
    "            \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Links data\n",
    "\n",
    "We saw last time that internal links, links to other pages on\n",
    "Wikipedia, are returned as a particular element of the JSON data\n",
    "returned by the MediaWiki API. I write a small helper funtion\n",
    "`links_as_list` (defined above) to extract this as a list in Python.\n",
    "Let's use this to get all of the links of the University of Richmond\n",
    "page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_wiki_json(\"University of Richmond\")\n",
    "links = links_as_list(data)\n",
    "links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, a reasonable next step would be to grab the data associated with\n",
    "each of these pages. To download the data for the first link we would\n",
    "just do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_wiki_json(links[0])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do this automatically for all of the links? We want to make use\n",
    "of a `for` loop. A for loop cycles through all of the elements of a\n",
    "list and applies a set of instructions to each element. \n",
    "\n",
    "Here's an example where we take each element in the list of links and\n",
    "print out just the first three letters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in links:\n",
    "    print(link[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to grab the webpage data for each link from the UR page,\n",
    "we can now just do this (this will take a while the first time you\n",
    "run it, but will be quick the second time):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in links:\n",
    "    get_wiki_json(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the MediaWiki data\n",
    "\n",
    "Now, finally, we have the code and functionality to look at a\n",
    "collection of Wikipedia pages. Let's start with a simple task\n",
    "of counting how many links all of the pages linked from the Richmond\n",
    "site have. Pay attention to how I do this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_links = []\n",
    "ur_links = links_as_list(get_wiki_json(\"University of Richmond\"))\n",
    "\n",
    "for link in ur_links:\n",
    "    data = get_wiki_json(link)\n",
    "    new_links = links_as_list(data)\n",
    "    num_links.append(len(new_links))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look at the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(num_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can we do with this? For starters, what's the average\n",
    "number of links on each page?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(num_links) / len(num_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does this compare to the number of links from the Richmond site?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ur_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice\n",
    "\n",
    "Take a look at the Wikipedia page on Rock and Roll Hall of\n",
    "Fame inductees:\n",
    "\n",
    "> https://en.wikipedia.org/wiki/List_of_Rock_and_Roll_Hall_of_Fame_inductees\n",
    "\n",
    "While some of the links on the page point to other references,\n",
    "most refer to the winners or inductors of the Hall of Fame.\n",
    "\n",
    "Below, write code that:\n",
    "\n",
    "1. Downloads all of the links from the Rock and Roll Hall of Fame\n",
    "Wikipedia page.\n",
    "2. Then, extract from each page all of the links from **that** page\n",
    "and puts them together in one appended list called `all_links`.\n",
    "3. Use the `Collections.counter` object to find the 20 links that\n",
    "are used most across all of the pages.\n",
    "4. Think about the most frequent 20 pages and try to reason why\n",
    "these are the most common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
